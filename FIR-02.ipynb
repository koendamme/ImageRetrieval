{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - _Foundations of Information Retrieval 2023_\n",
    "\n",
    "The second assignment is divided in two parts and it is due on October 29th at 23:59, via Canvas (no deadline extentions).\n",
    "The first part is about _learning-to-rank_ exercises and the use of a regression and SVM for ranking. You will work on a data set of document, for which two features are extracted. The second part contains small practical image retrieval exercises using the Bag-of-Words framework, using images of the city of London.\n",
    "\n",
    "### Background information\n",
    "Please study the materials provided with Lecture 4 (for Part 01) and Lecture 6 (for Part 02) to have a solid theoretical base to solve the following exercises. \n",
    "\n",
    "To simplify many operations, you can use Python libraries for Machine Learning (for Part 01 and Part 02) and Image Processing (only for Part 02). We advise you to familiarize with:\n",
    "* [scikit-learn](): library for Machine Learning\n",
    "* [scikit-image](): library of functions for image processing \n",
    "* [matplotlib](): for visualizations and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install needed libraries and tools\n",
    "The required libraries are already installed in the virtual machine system.\n",
    "If you run the notebook on your machine, you can install the libraries by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "# Scikit-image library for image processing tools in python\n",
    "!pip3 install scikit-image\n",
    "# Scikit-learn library for ML in python\n",
    "!pip3 install scikit-learn\n",
    "# install the library h5py (needed to load and read the ground truth relevance file of assignment 02)\n",
    "!pip3 install h5py\n",
    "# install the library tqdm (for progress visualization)\n",
    "!pip3 install tqdm\n",
    "# Matplotlib/Pylab for plots and visualization in python\n",
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 01: _Learning-to-rank_\n",
    "\n",
    "Ranking problems are formulated for data sets that consist of lists of items with some ordering specifications. The order specifications are usually defined by giving a numerical score or a binary judgement (relevant/non-relevant) for each item so that for any two samples `x` and `y`, either `x < y`, `x > y` or `b` and `a` are not comparable. See the lecture material for more details.\n",
    "\n",
    "### Data set and preparation\n",
    "You will work with a synthetic data set (with no specific domain) and focus on the methodological aspects of learning-to-rank methods.\n",
    "\n",
    "The data set contains $300$ documents (for which, 2-element feature vectors are computed, i.e. each document is represented by two features), divided into 210 training and 90 test samples. An ordering rule, defined by given coefficients, is applied to generate the samples in a ranked way and simulate the characteristics of a ranking problem. For each sample in the data set, a target value is specified as a label from the set Y = {0, 1, 2}. \n",
    "\n",
    "The training set is built in such a way that there are two blocks of data, which represent two sets of documents that compose our collection.\n",
    "\n",
    "In the following we provide the code to load and visualize the data (and the train/test split). The two groups of training data are marked by flags in the variable `distID_train`, which contains a 0 if the corresponding document is in the first set, and a 1 otherwise. Eventually explore more deeply the data structures and their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pylab as pl\n",
    "from sklearn import svm, linear_model, model_selection\n",
    "import pickle\n",
    "\n",
    "\n",
    "# load pre-computed descriptors\n",
    "f = open('data02/data-ltr.bin', 'rb')\n",
    "dataset = pickle.load(f)\n",
    "distID_train = dataset['distID_train'] \n",
    "X_train = dataset['X_train']\n",
    "y_train = dataset['y_train']\n",
    "X_test = dataset['X_test']\n",
    "y_test = dataset['y_test']\n",
    "w = dataset['w']\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the training data points\n",
    "To have a better idea of the data set, we can plot them. \n",
    "\n",
    "For visualization purposes, we use different colors to mark the target class, and different marker shapes to indicate the block to which the samples belong, i.e. a triangle for block 1 and a circle to for block 2. The blocks refer to the documents associated to query 1 and query 2, respectively.\n",
    "\n",
    "The horizontal gray line is the ground truth _ranking vector (direction)_. See slides of the lecture ('Ordinal regression').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Plot the training samples\n",
    "idx = (distID_train == 0)  # select the samples from the first distribution centered in (0,0)\n",
    "pl.scatter(X_train[idx, 0], X_train[idx, 1], \n",
    "           c=y_train[idx], marker='^', \n",
    "           cmap=pl.cm.Paired, s=100, edgecolors='k')\n",
    "\n",
    "# ~idx contains the indices of the samples from distribution 2\n",
    "pl.scatter(X_train[~idx, 0], X_train[~idx, 1], \n",
    "           c=y_train[~idx], marker='o', \n",
    "           cmap=pl.cm.Paired, s=100, edgecolors='k')\n",
    "\n",
    "# Plot the ground truth ranking line\n",
    "x_space = np.linspace(X_train[:,0].min(), X_train[:,0].max())\n",
    "pl.plot(x_space * w[0], x_space * w[1] + X_train[:,1].mean(), color='gray')\n",
    "\n",
    "cm = pl.cm.get_cmap('Paired')\n",
    "pl.legend(loc='lower right',\n",
    "          handles=[mpatches.Patch(facecolor=cm(0.), edgecolor='k', label='Rank 0'),\n",
    "                  mpatches.Patch(facecolor=cm(0.5), edgecolor='k', label='Rank 1'),\n",
    "                  mpatches.Patch(facecolor=cm(1.), edgecolor='k', label='Rank 2'),])\n",
    "\n",
    "#pl.axis('equal')\n",
    "pl.title('Training data')\n",
    "pl.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to notice how for both blocks of data, there is a ranking vector. This means that, if we project the samples onto this vector, we have an ordered list of the documents (note that the vector is direction onto which we can project the document samples and achieve a ranking) - see the lecture slides for details about ordinal regression.\n",
    "\n",
    "Thus, using ML techniques to estimate the coefficients of this vector from the training data qualifies as a `learning-to-rank` approach.\n",
    "\n",
    "> _note:_ we could plot the vector bacause we know the ground truth coefficients $w$, as we used them to generate the data. In a real-world case, you do not know these coefficients and need to estimate them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the test data points\n",
    "Plot of the test data points. Note that they are drawn from a distribution of the same shape of the two parts of the training data, but centered in a different region of the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of test data\n",
    "pl.scatter(X_test[:, 0], X_test[:, 1], \n",
    "           c=y_test, marker='^', \n",
    "           cmap=pl.cm.Paired, s=100, edgecolors='k')\n",
    "\n",
    "#pl.axis('equal')\n",
    "pl.title('Test data')\n",
    "pl.show()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data and test data\n",
    "For visualization purpose, below the training data points (circles and triangles of light blue, light orange and brown color) and test data (squares of light green, blue and dark grey color) are show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of training data\n",
    "idx = (distID_train == 0)  # select the samples from the first distribution centered in (0,0)\n",
    "pl.scatter(X_train[idx, 0], X_train[idx, 1], \n",
    "           c=y_train[idx], marker='^', \n",
    "           cmap=pl.cm.Paired, s=100)\n",
    "\n",
    "# ~idx contains the indices of the samples from distribution 2\n",
    "pl.scatter(X_train[~idx, 0], X_train[~idx, 1], \n",
    "           c=y_train[~idx], marker='o', \n",
    "           cmap=pl.cm.Paired, s=100)\n",
    "\n",
    "# Plot of test data\n",
    "pl.scatter(X_test[:, 0], X_test[:, 1], \n",
    "           c=y_test, marker='s', \n",
    "           cmap=pl.cm.Accent, s=100, edgecolors='k')\n",
    "\n",
    "#pl.axis('equal')\n",
    "pl.title('Training and test data')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01.A: _(Pointwise) Regression-based ranking_\n",
    "\n",
    "We can implement a regression-based approach to estimate the coefficients of a ranking model (the vector w) and use these coefficients to rank the documents in the test set. We want to learn the parameters of a model using an optimization method, so that we can use it to rank test documents given their feature-vector representation.\n",
    "\n",
    "__1(a). Use the fecture vectors in the training set `X_train` and the corresponding labels `y_train` to learn a regression model. Save the estimated coefficients in the variable `coef` (a vector of 2 elements, because we have 2 features), which is used by the visualization code in the following notebook cell.__\n",
    "\n",
    "\n",
    "> _Hint:_ look into the [Linear Models](https://scikit-learn.org/stable/modules/linear_model.html) class of methods in scikit-learn.\n",
    ">\n",
    "\n",
    "__1(b).Use different regression models and compare the data points and resulting ranking vector.__ \n",
    "We provide you with the code for plotting the results. Feel free to modify it (f.i. if you want to plot the ranking vector computed with more than one model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# your code here\n",
    "## BEGIN ANSWER\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "ridge = linear_model.Ridge(alpha=.5)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "lasso = linear_model.Lasso(alpha=1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "coef = lasso.coef_\n",
    "\n",
    "\n",
    "## END ANSWER\n",
    "\n",
    "# Remember to fill in the variable `coef` with the 2 estimated coefficients of the vector. \n",
    "# We use the `coef` variable for plotting purposes in the following cell\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells plots the training data points and the ranking vectors.\n",
    "\n",
    "The red vector indicates the ground truth direction that we should aim at estimate (which in our case provides the optimal ranking according to the relevance judgements). The grey vector is the result of the estimation using a regression-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training samples and the estimated ranking model (vector w)\n",
    "# NOTE: we use the block indices (idx and ~idx) computed before only for a visualization purpose\n",
    "pl.scatter(X_train[idx, 0], X_train[idx, 1], \n",
    "           c=y_train[idx], marker='^', \n",
    "           cmap=pl.cm.Paired, s=100, edgecolors='k')\n",
    "\n",
    "# ~idx contains the indices of the samples from distribution 2\n",
    "pl.scatter(X_train[~idx, 0], X_train[~idx, 1], \n",
    "           c=y_train[~idx], marker='o', \n",
    "           cmap=pl.cm.Paired, s=100, edgecolors='k')\n",
    "\n",
    "# plot the estimated ranking vector (line)\n",
    "x_space = np.linspace(X_train[:,0].min(), X_train[:,0].max())\n",
    "pl.plot(x_space * coef[0], x_space * coef[1] + X_train[:,1].mean(), color='gray')\n",
    "\n",
    "# plot ground truth coefficients (those used for data generation)\n",
    "x_space = np.linspace(X_train[:,0].min(), X_train[:,0].max())\n",
    "pl.plot(x_space * w[0], x_space * w[1] + X_train[:,1].mean(), color='red')\n",
    "\n",
    "\n",
    "\n",
    "#pl.axis('equal')\n",
    "pl.title('Estimation by linear regression')\n",
    "\n",
    "cm = pl.cm.get_cmap('Paired')\n",
    "pl.legend(loc='lower right',\n",
    "          handles=[mpatches.Patch(facecolor=cm(0.), edgecolor='k', label='Rank 0'),\n",
    "                  mpatches.Patch(facecolor=cm(0.5), edgecolor='k', label='Rank 1'),\n",
    "                  mpatches.Patch(facecolor=cm(1.), edgecolor='k', label='Rank 2'),])\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Make your own observations on the visual results that you got above, including the cases when you used different regression models. Answer as a comment in the cell below.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer below\n",
    "## BEGIN ANSWER\n",
    "# We tried two different methods to find the coefficients of the ranking vector.\n",
    "# First, we used both queries to determine the coefficients, but this lead to a diagonal line with linear regression\n",
    "# and ridge regression. However, when using lasso and setting alpha to 1, it resulted in a horizontal line.\n",
    "# We are not sure why Lasso results in a horizontal line while linear and ridge regression do not.\n",
    "# Next, we used only one query to determine the coefficients. This lead to horizontal lines for all models.\n",
    "## END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01.B: _performance of regression-based ranking (Kendall's Tau)_\n",
    "\n",
    "Evaluate the performance of the learned model by using it to rank the test set and comparing it to the target labels.\n",
    "As the problem you are working with is a ranking problem, you need to use a measure of performance that is based on a ranking score, that is you need to compare the _order_ of the retrieved documents by your model with the _order_ of the ground truth data.\n",
    "\n",
    "In the lecture, we have seen that the [Kendall's tau correlation coefficient](http://en.wikipedia.org/wiki/Kendall_tau_rank_correlation_coefficient) is a measure of the quality of data ordering with respect to a given ground truth ordering. It is defined as (P - Q)/(P + Q), where P is the number of concordant pairs and Q is the number of discordant pairs.\n",
    "\n",
    "__1. Compute the Kendall's Tau on the ranking order that you obtain by applying your regression model to the documents in the test set `X_test` with labels `y_test`.__\n",
    "\n",
    "> Look into [Scipy's Stats library](https://docs.scipy.org/doc/scipy/reference/stats.html) for hints on how to compute the [Kendall's tau](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html#scipy.stats.kendalltau). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN ANSWER\n",
    "X_rank = np.zeros((X_test.shape[0], 2))\n",
    "\n",
    "coef = reg.coef_\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_rank[i, :] = np.dot(X_test[i, :], coef) / np.linalg.norm(coef) * coef / np.linalg.norm(coef)\n",
    "\n",
    "pl.scatter(X_rank[:, 0], X_rank[:, 1], c=y_test)\n",
    "\n",
    "tao_1 = stats.kendalltau(X_rank[:, 0], y_test)\n",
    "\n",
    "print(tao_1.statistic)\n",
    "\n",
    "\n",
    "## END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Compute the Kendal's Tau of the ranking order that you obtain by applying your regression model and that you can obtain by applying the ground truth ranking vector $w$. Make your observations below.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## BEGIN ANSWER\n",
    "tao_2 = stats.kendalltau(X_test[:, 0], y_test)\n",
    "print(tao_2)\n",
    "## END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your observations below\n",
    "## BEGIN ANSWER\n",
    "# We can see that the Kendel's Tau values are exactly the same. This is because our ranking vector is almost equal to the ground truth vector and therefore they have the same amount of concordant and discordant pairs. If we compute the Kendel's Tau value on the ranking vector computed with regular linear regression, which resulted in a more diagonal vector, the value became much lower (0.5516379557934487).\n",
    "\n",
    "## END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers for ranking problems \n",
    "\n",
    "A ranking problem can be transformed into a classification problem by manipulating the training data (feature vectors and ordinal labels). One can transform the training data by computing all the pairwise differences of the feature vectors $x_i$ and corresponding labels $y_i$ as:\n",
    "\n",
    "$(\\mathbf{x'_k}, y'_k) = \\left(\\mathbf{x_i} - \\mathbf{x_j}, sign(y_i - y_j)\\right)$\n",
    "\n",
    "In the case we have $N$ documents (represented by $N$ feature vectors), the pairwise transformed data will be composed by $N(N-1)/2$ feature vectors (and corresponding labels). In this way, the input training data with multiple ordinal targets are transformed into a two-class data set, suitable for linear classification algorithms.\n",
    "\n",
    "## _Pairwise transform_\n",
    "As seen in the lecture, the pairwise transform translates your dataset of single documents (with labels) into a dataset of pairs of documents (with labels related to the pairs - see Lecture 05 slides). You will use (in later parts of the assignment) the dataset that results from the pairwise transformation to train a ranking model (using an SVM classifier).\n",
    "\n",
    "Here, we provide the code to perform the pairwise transform of the training data. Read and understand it, before using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form all pairwise combinations\n",
    "comb = itertools.combinations(range(X_train.shape[0]), 2)\n",
    "k = 0\n",
    "X_pw, y_pw, diff = [], [], []\n",
    "for (i, j) in comb:\n",
    "    #print(i, j, y_train[i], y_train[j])\n",
    "    if y_train[i] == y_train[j] \\\n",
    "        or distID_train[i] != distID_train[j]:\n",
    "        # skip if same target or different group\n",
    "        continue\n",
    "    X_pw.append(X_train[i] - X_train[j])\n",
    "    diff.append(y_train[i] - y_train[j])\n",
    "    y_pw.append(np.sign(diff[-1]))\n",
    "    # output balanced classes\n",
    "    if y_pw[-1] != (-1) ** k:\n",
    "        y_pw[-1] *= -1\n",
    "        X_pw[-1] *= -1\n",
    "        diff[-1] *= -1\n",
    "    k += 1\n",
    "    \n",
    "X_pw, y_pw, diff = map(np.asanyarray, (X_pw, y_pw, diff))\n",
    "# Plot the transformed data set \n",
    "# (note that the number of points -- documents -- increases \n",
    "# due to the all the pairwise combination of documents)    \n",
    "\n",
    "# We use X_pw and y_pw to indicate the transformed data points and the corresponding labels, respectively\n",
    "pl.scatter(X_pw[:, 0], X_pw[:, 1], c=diff, s=60, \n",
    "           marker='o', edgecolors='k', cmap=pl.cm.Paired)\n",
    "\n",
    "x_space = np.linspace(-10, 10)\n",
    "pl.plot(x_space * w[1], 3* x_space * w[0], color='gray')  # this is the expected separation plane (from the ground truth coefficients)\n",
    "pl.title('Pairwise transformed training data')\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01.C: _ranking SVM_\n",
    "\n",
    "After pairwise transformaion, the classification problem looks (almost) linearly separable, i.e. a line can be drawn that separates the two sets of document pair points. This is not always the case - however, you can still use an SVM then, allowing for errors).\n",
    "\n",
    "\n",
    "### SVM implementation\n",
    "__1. Write the code to train a ranking SVM model on the pairwise transformed data. The model will estimate the coefficients of the ranking vector (we call if `coef` in the code below), which you can use to rank the test documents as explained above.__\n",
    "\n",
    "Study the SVM from the lecture materials and from [online resources](https://en.wikipedia.org/wiki/Support_vector_machine).\n",
    "Read the documentation of the [Scikit-learn SVM package](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) (the attribute `coef_` will turn to be useful).\n",
    "\n",
    "> _Hint:_ explore the impact of parameter C of the SVM on the classification results. Look into the generalization capabilities of the model.\n",
    ">\n",
    "> We provide the code for generation of the plot. If you need, you can modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "## BEGIN ANSWER\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear', C=.2)\n",
    "svc.fit(X_pw, y_pw)\n",
    "\n",
    "coef = svc.coef_[0]\n",
    "## END ANSWER\n",
    "\n",
    "# value of the estimated coefficients\n",
    "print('Estimated coefficients: ', coef)\n",
    "print('Ground truth coefficients: ', w)\n",
    "\n",
    "# plot the training data and the coefficient vector \n",
    "pl.scatter(X_train[idx, 0], X_train[idx, 1], \n",
    "           c=y_train[idx], marker='^', \n",
    "           cmap=pl.cm.Paired, s=100, edgecolors='k')\n",
    "\n",
    "# ~idx contains the indices of the samples from distribution 2\n",
    "pl.scatter(X_train[~idx, 0], X_train[~idx, 1], \n",
    "           c=y_train[~idx], marker='o', \n",
    "           cmap=pl.cm.Paired, s=100, edgecolors='k')\n",
    "\n",
    "\n",
    "# plot ground truth coefficients (those used for data generation)\n",
    "x_space = np.linspace(X_train[:,0].min(), X_train[:,0].max())\n",
    "pl.plot(x_space * w[0], x_space * w[1] + X_train[:,1].mean(), color='red')\n",
    "\n",
    "# plot the estimated ranking vector (line)\n",
    "x_space = np.linspace(X_train[:,0].min(), X_train[:,0].max())\n",
    "pl.plot(x_space * coef[0], x_space * coef[1] + X_train[:,1].mean(), color='gray')\n",
    "\n",
    "pl.title('Estimation by ranking SVM')\n",
    "cm = pl.cm.get_cmap('Paired')\n",
    "pl.legend(loc='lower right',\n",
    "          handles=[mpatches.Patch(facecolor=cm(0.), edgecolor='k', label='Target 0'),\n",
    "                  mpatches.Patch(facecolor=cm(0.5), edgecolor='k', label='Target 1'),\n",
    "                  mpatches.Patch(facecolor=cm(1.), edgecolor='k', label='Target 2'),])\n",
    "\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Make your own observations on the visual results that you got above, eventually including the cases when you used SVMs trained with different C values. Answer as a comment in the cell below.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your observations below, as a comment\n",
    "## BEGIN ANSWER\n",
    "# The C value of the SVM does not seem to have much influence on the resulting projection vector. We think this is because the data is balanced, meaning that when we are allowing for misclassifications, roughly an equal amount will occur on each side of the line keeping it in the same direction. It is also again close to the ground truth.\n",
    "## END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01.E: _performance of ranking SVM_\n",
    "\n",
    "__1. Evaluate the Kendall's Tau score of the ranking SVM Model applied to the test documents. Compare the results with the regression models and make your own observations (you can write them as comments in the cell below, or you can create a new cell).__\n",
    "\n",
    "> Note that the SVM predicts a binary class, which will contrast with the ordinal prediction required to compute the Kendall's tau. You should thus use the vector of coefficients learned by the ranking SVM to perform the projections of the test samples and obtain an ordinal categorization. To perform the projection operation you may want to use the [NumPy dot product function](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.dot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "## BEGIN ANSWER\n",
    "X_rank = np.zeros((X_test.shape[0], 2))\n",
    "\n",
    "coef = svc.coef_[0]\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_rank[i, :] = np.dot(X_test[i, :], coef) / np.linalg.norm(coef) * coef / np.linalg.norm(coef)\n",
    "\n",
    "pl.scatter(X_rank[:, 0], X_rank[:, 1], c=y_test)\n",
    "\n",
    "tao_1 = stats.kendalltau(X_rank[:, 0], y_test)\n",
    "\n",
    "print(tao_1.statistic)\n",
    "## END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Compute the Kendal's Tau of the ranking order obtained by applying your SVM model when compared to that you obtain by applying the ground truth ranking vector $w$. Make your observations below.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN ANSWER\n",
    "tao_2 = stats.kendalltau(X_test[:, 0], y_test)\n",
    "print(tao_2.statistic)\n",
    "## END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your observations below\n",
    "## BEGIN ANSWER\n",
    "# We have the same observation as before. Because our projection vector is very close to the ground truth, we have the same amount of concordant and discordant pairs, resulting in an equal Kendal's Tau.\n",
    "## END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 02: Image Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_You are suggested to work on this part after Lecture 06 (Multimedia Information Retrieval - Image Retrieval)_\n",
    "\n",
    "In the second part of the assignment, you will work on a simple image retrieval problem, and implement parts of the Bag Of Words retrieval approach. You will work with images from the Mapillary Street-Level Sequences  (MSLS) data set. For an autonomous car, for instance, it is important to retrieve from a map or reference set of images the most similar images to what is being recording with a camera, in order to recognize a previously visited place.\n",
    "\n",
    "You are asked to retrieve, from a collection of city images (we work with images from London), the most similar images to given queries. For this, we provide you a dataset, composed of:\n",
    "\n",
    "* _database/_ folder: contains XXX images used as map (document collection)\n",
    "* _query/_ folder: contains XXX images used as queries\n",
    "* _database/database_lite.json_: list of images included in the map (the order is important because it correspond to the orded in the similarity matrix - see below)\n",
    "* _query/query_lite.json_: list of images used for query (the order is important because it correspond to the orded in the similarity matrix - see below)\n",
    "* _london_lite_gt.h5_: matrix of image similarity, containts all the information about relevant/non-relevant images \n",
    "\n",
    "__Relevance judgements (in the form of a matrix, the structure of which is explained below)__\n",
    "For each query image, we provide a specification of which map images are relevant (1 in the london_lite_gt.h5 file) and which are not (0 in the london_lite_gt.h5). The content of the '.h5' file is a matrix, with rows corresponding to query images and columns corresponding to the map images. The matrix has thus has $N=500$ rows (where $N$ is the number of query images in the folder `query/images/`) and $M=1000$ columns (where $M$ is the number of query images in the folder `database/images/`). \n",
    "\n",
    "The $i$-th row of the matrix thus contains the relevance judgement for the $i$-th query image with respect to the map images. For instance, row 5 of the matrix is a vector of 1000 elements (one for each image in the map), in which each element is 1 if the corresponding map image is relevant for the query image 5, or $0$ in case it is not relevant.\n",
    "\n",
    "Please note that in Python the index of a vector (or matrix) starts from 0 (instead of 1). This means that the first element of a vector has index $0$, and that, for example, the element with index $25$ corresponds to the 26-th element of the vector. \n",
    "Thus, as an example, the cell $(10, 25)$ contains the relevance of the 26-th map image for the $11$-th query image (i.e. is the 26-th image in the map relevan (1) or not (0) for the query image number 11?).\n",
    " \n",
    "\n",
    "In the following, we provide you the code for loading the data set and to visualize the images.\n",
    "\n",
    "\n",
    "> Note: for this assignment we use the '\\_lite' version of the map and query set, which include a reduced number of images. This is to simplify the computations required to work on this assignment. For the final project, you can work using the full-sized sets of images from London. This will require longer computations, but you can obtain more reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the query and map images\n",
    "In the following we load the list of images in the map set (collection) and in the query set.\n",
    "For each image, we have information also about the absolute coordinates of the camera that took the picture in the city of London. _Do not consider these coordinates for the exercises. We use them only to show the trajectory followed by the camera taking the pictures (see below)._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "# map\n",
    "with open(\"data02/database/database_lite.json\",\"r\") as f:\n",
    "    m_idx = json.load(f)\n",
    "    m_imgs = np.array(m_idx[\"im_paths\"])\n",
    "    m_loc=np.array(m_idx[\"loc\"])\n",
    "\n",
    "# query\n",
    "with open(\"data02/query/query_lite.json\",\"r\") as f:\n",
    "    q_idx=json.load(f)\n",
    "    q_imgs=np.array(q_idx[\"im_paths\"])\n",
    "    q_loc=np.array(q_idx[\"loc\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of image locations\n",
    "The pictures in the map (collection) and query sets have been recorded in different parts of the city of London. The map images are indicated by the blue trajectory (run the code below), while the query images are indicated by the orange trajectory segments.\n",
    "\n",
    "You can notice how the blue and orange trajectories overlap partially. In that overlapping area, the query images will have some relevant images among the map images. Conversely, in the areas there there is no overlap (and the points are not even close), the map images will be non-relevant for the query images. \n",
    "Please read more details in the next sections, where the ground truth relevance judgements are provided and explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(m_loc[:,0],m_loc[:,1], label=\"Map\")\n",
    "plt.scatter(q_loc[:,0],q_loc[:,1], label=\"Query\", s=0.5)\n",
    "#plt.axis(\"equal\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the similarity matrix (relevance judgements)\n",
    "\n",
    "The london_lite_gt.h5 file contains two sets of relevance  judgements, specified by the keys `sim` and `fov`.\n",
    "`sim` specifies it in the form 0/1 (non-relevant/relevant).\n",
    "`fov` (that stands for field of view) specifies it as a degree of similarity, a value defined in the interval $[0, 1]$ that indicates how much the query and map images are similar. We do not use it for this assignment, but you may consider it when working on the final project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the relevance judgements\n",
    "with h5py.File(\"data02/london_lite_gt.h5\",\"r\") as f:\n",
    "    fovs = f[\"fov\"][:]\n",
    "    sim = f[\"sim\"][:].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the matrix with binary relevance judgements `sim` in this assignment. \n",
    "To obtain the relevance judgement of the map image 'map_id' for the query 'query_id', you  can use the syntax `sim[query_id, map_id]`. \n",
    "\n",
    "In the following we show few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that for the 10th map image we have to access column 10-1=9\n",
    "print('Relevance of the 10th map image for the 5th query image: ', sim[4, 9])\n",
    "# similarly for the 221th query image we access column 221-1=220\n",
    "print('Relevance of the 221th map image for the 151th query image: ', sim[150, 220])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of relevant images per query\n",
    "\n",
    "The majority of query images have between $0$ and $10$ relevant map images (ground truth positive matches). A few query images have no relevant map images (no positive matches) in the database. A few others have up to $30$ positive database images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.sum(sim, axis=1),bins=50)\n",
    "plt.xlabel('Image count')\n",
    "plt.ylabel('Relevant map images')\n",
    "plt.title('Distribution of relevant images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of random images. \n",
    "For a given query image, we select one relevant and one non-relevant image from the map.\n",
    "> By running several time the following cell, you can see how some of the relevant images are very difficult cases. This can give an idea of how complicate is to perform robust image retrieval.\n",
    "This is also an indication that the retrieval results that you will compute will not be high (do not worry about them, then - focus on the methodology and discussions).\n",
    "\n",
    "See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random index for the query image\n",
    "# You can try with it (but in the case a query image does not have relevant results in the map, the code will give an error)\n",
    "query_idx = np.random.randint(0, 499) \n",
    "\n",
    "# For visualization purposes, we select a query image for which there are relevant and non-relevant results in the collection (map)\n",
    "# query_idx = 120\n",
    "\n",
    "# select the relevant and non-relevant map images for the randomly selected query image\n",
    "rel = np.where(sim[query_idx, :] == 1)\n",
    "nonrel = np.where(sim[query_idx, :] == 0)\n",
    "\n",
    "\n",
    "# randomly select a relevant and non-relevant image\n",
    "rel_idx = rel[0][np.random.randint(0, len(rel[0]) - 1)]\n",
    "nonrel_idx = nonrel[0][np.random.randint(0, len(nonrel[0]) - 1)]\n",
    "\n",
    "plt.figure(figsize=(17,10))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(plt.imread('data02/' + q_imgs[query_idx]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title('Relevant image result (from map)')\n",
    "plt.imshow(plt.imread('data02/' + m_imgs[rel_idx]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title('Non-relevant image result (from map)')\n",
    "plt.imshow(plt.imread('data02/' + m_imgs[nonrel_idx]))\n",
    "plt.axis(\"off\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to collect all relevance judgement for the $i$-th query image, you need to access the (i-1)-th row of the matrix `sim`. To visualize the relevance judgment you have to loop over the accesses vector (a row of a matrix is a vector.\n",
    "\n",
    "In the following an example of how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing (and printing out) the relevance judgement for the query image number 16\n",
    "\n",
    "query_id = 15  # 16-1\n",
    "for idx in range(len(sim[query_id, :])):\n",
    "    # idx is the index of the vector. It corresponds to the map image index\n",
    "    # rel contains the relevance (0/1) of the idx-th map image for the considered query image (9th image in our case)\n",
    "    print(\"Relevance of the %d-th map image for the 16-th query image: %d\" % (idx, sim[query_id, idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT: Bag of Words representation\n",
    "As seen in the lecture, in order to construct a dictionary of 'visual' terms (words) from a collection of images, one can extract local features from the images (e.g. keypoints) and use a clustering algorithm to automatically find groups (i.e. clusters) of similar local features. The centroid of a cluster is also a feature vector and can be considered as a term to be used for the search and retrieval of similar images.\n",
    "\n",
    "In this assignment, we use the [$K$-Means clustering algorithm](https://www.youtube.com/watch?v=_aWzGGNrcic) (where $k$ is defined as a parameter by us) to extract the visual dictionary of terms from the images in the map collection (in the cells below you will find more information).\n",
    "\n",
    "In the following code (please study it), we:\n",
    "* load the list of images in the map collection\n",
    "* for each image we extract a maximum of 50 ORB keypoints (local keypoint features). Each keypoint is described by a vector of 256 elements (keypoint descriptor) that describes the local characteristics of the image.\n",
    "* use the set of keypoint descriptors (50 keypoints * 1000 images = 50000 vectors) as input of the k-Means clustering algorithm\n",
    "* compute k=32 cluster centroids (which correspond to the 32 terms of our dictionary) - k is a configurable parameter\n",
    "\n",
    "> __Note:__ Please, find more information about ORB in the [ORB paper](https://ieeexplore.ieee.org/document/6126544) and in the [Scikit Image ORB documentation](https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_orb.html).\n",
    "\n",
    "> __Note 2:__ it may take some time to extract the descriptors from all images. Once extracted you can save them, and reload them when needed. If you change the number of centroids k, then you need to recompute the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage\n",
    "from skimage.feature import ORB, SIFT\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "# Initialize the ORB descriptor\n",
    "descriptor_extractor = ORB(n_keypoints=50)\n",
    "# Initialize the data structure that will contain all the descriptors\n",
    "descriptors = None\n",
    "\n",
    "# Loop over map images\n",
    "for img_name in m_imgs:\n",
    "    #img = Image.open(os.path.join('data_image_retrieval/', img_name)).convert()\n",
    "    #img = np.asarray(img)\n",
    "    img = plt.imread(os.path.join('data02/', img_name))\n",
    "    img = rgb2gray(img)\n",
    "\n",
    "    # Extract ORB descriptors\n",
    "    descriptor_extractor.detect_and_extract(img)\n",
    "    # keypoints1 = descriptor_extractor.keypoints  # position of the points (not interesting for us)\n",
    "    descriptors_img = descriptor_extractor.descriptors  # descriptors (the feature vectors)\n",
    "\n",
    "    # Accumulate the computed descriptors\n",
    "    if descriptors is None:\n",
    "        descriptors = descriptors_img\n",
    "    else:\n",
    "        descriptors = np.vstack( (descriptors, descriptors_img))\n",
    "    #print(descriptors.shape)\n",
    "\n",
    "# We have extracted 50 descriptors per image, for 1000 images. The ORB descriptors have length 256\n",
    "print(descriptors.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save or load descriptors (comment/uncomment the code accordingly)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save descriptors (uncomment if you want to save the computed descriptors)\n",
    "f = open('data02/ORB-descriptors-map.bin', 'wb')\n",
    "data = pickle.dump(descriptors, f)\n",
    "f.close()\n",
    "\n",
    "# load pre-computed descriptors\n",
    "f = open('data02/ORB-descriptors-map.bin', 'rb')\n",
    "descriptors = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Following the Bag of Words framework (see the slides of the lecture), we can define the basic 'words' that compose the images via a clustering algorithm that takes the descriptors of the map images (training) as input. The basic 'words' form a so called visual vocabulary and are learned by grouping together those descriptor vectors that are most similar, i.e. their distance is lower than that between other descriptors.\n",
    "\n",
    "We do it by using the $K$-Means algorithm: find [here](https://en.wikipedia.org/wiki/K-means_clustering) some more information about KMeans and [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) the reference documentation of the algorithm in Scikit-learn. Also an explanatory video with examples [here](https://www.youtube.com/watch?v=_aWzGGNrcic).\n",
    "\n",
    "> Note: running the clustering may take few minutes. Once computed, you can save the centroids and subsequently load tham before use. If you change the value of K, you need to recompute the clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# clustering\n",
    "K = 32  # number of clusters (equivalent to the number of words) we want to estimate\n",
    "num_initialization = 5 # Number of time the k-means algorithm will be run with different centroid seeds.\n",
    "\n",
    "# Run the k-means clustering\n",
    "kmeans = KMeans(n_clusters=K, random_state=0, n_init=num_initialization, verbose=1)\n",
    "clusters = kmeans.fit(descriptors)  # we use the descriptors extracted from the map (training) images before\n",
    "centroids = clusters.cluster_centers_\n",
    "\n",
    "print(\"Shape of the centroids matrix: \", centroids.shape)\n",
    "print(\"We computed \", centroids.shape[0], \"centroids of lengh \", centroids.shape[1], \" (the same of the descriptor)\")\n",
    "# Rememeber: the centroids can be considered as the words that compose our documents \n",
    "# -> in this case the basic components of the images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 02.A: _'Bag of words' vector computation_\n",
    "\n",
    "In order to compute the BoW representation of an image, we have to count the occurrences of the visual 'words', i.e. the local features, that we have learned in our visual vocabulary (using the $K$-Means clustering).\n",
    "\n",
    "Given an image, we do the following:\n",
    "   * we extract the local descriptors (we are using ORB descriptors in this assignment) \n",
    "   * for each descriptor we find the closest 'word' (i.e. the closes cluster centroid) in the visual vocabulary\n",
    "   * we sum +1 to the counting of the occurence of the 'word' (in this way we construct the histogram of the occurrence of the visual 'words')\n",
    "    \n",
    "An image is thus described with a vector (of $K$ elements) that is an histogram of the occurrence of the words in the learned visual vocabulary (similar to the occurrence of words in a text document).  \n",
    "\n",
    "In the following, implement the core of the function that computes the BoW vector. The function takes as input the centroids of the learned clusters (i.e. the vocabulary of the visual words) and the set of descriptors extracted from an image. It must return the BoW vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import skimage\n",
    "from skimage.feature import ORB\n",
    "from skimage.color import rgb2gray\n",
    "descriptor_extractor = ORB(n_keypoints=50)\n",
    "\n",
    "\n",
    "# compute the bag of word vector for an image\n",
    "def bag_of_words(centroids, img_descriptors):\n",
    "    n_centroids = centroids.shape[0]  # number of centroids found with the KMeans clustering\n",
    "    n_descriptors = img_descriptors.shape[0]  # number of descriptors extracted from the image\n",
    "    \n",
    "    # initialization of the bag of words (BoW) vector\n",
    "    # Note that the BoW vector has length equal to the number of cluster centroids\n",
    "    # The cluster centroids are indeed our visual words, and the BoW will be the histogram of these words found in the given image\n",
    "    bow_vector = np.zeros(n_centroids)  \n",
    "    \n",
    "    for i in range(n_descriptors):\n",
    "        ## BEGIN ANSWER\n",
    "        idx = np.argmin(np.linalg.norm(centroids - img_descriptors[i], axis=1), axis=0)\n",
    "        bow_vector[idx] += 1\n",
    "        ## END ANSWER\n",
    "    return bow_vector\n",
    "\n",
    "\n",
    "# Test the implementation of the BoW vector computation\n",
    "img = plt.imread(os.path.join('data02/', q_imgs[0]))\n",
    "img = rgb2gray(img)\n",
    "\n",
    "descriptor_extractor.detect_and_extract(img)  \n",
    "# keypoints1 = descriptor_extractor.keypoints  # position of the points (not interesting for us)\n",
    "query_img_descriptors = descriptor_extractor.descriptors \n",
    "\n",
    "bow = bag_of_words(centroids, query_img_descriptors)\n",
    "print(\"Size of the bow vector: \", bow.shape)\n",
    "print(\"Bow vector: \", bow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words representation of the map images \n",
    "Given a query image, our aim is to find similar images in our collection (i.e. map) images.\n",
    "To do that, we have to convert the query image into its Bag of words (BoW) representation and look for similar vectors in the collection (i.e. map) images.\n",
    "\n",
    "At this stage, since we will do many queries, it is handy to compute the BoW vectors for all images in the collection (i.e. map) and reuse them for multiple queries. (We convert all images in the collection to their BoW representation).\n",
    "\n",
    "> It is advised to study and understand the code below (it may take some time to run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_map_images = None\n",
    "# loop over the images in the map set\n",
    "for img_name in tqdm(m_imgs):\n",
    "    # load image\n",
    "    img = plt.imread(os.path.join('data02/', img_name))\n",
    "    img = rgb2gray(img)\n",
    "    \n",
    "    # extract the keypoints and corresponding descriptors (50 ORB descriptors)\n",
    "    descriptor_extractor.detect_and_extract(img)\n",
    "    img_descriptors = descriptor_extractor.descriptors  # descriptors (the feature vectors)\n",
    "    \n",
    "    # compute BoW representation of the image (using the basic 'words', i.e. centroids, computed earlier)\n",
    "    bow = bag_of_words(centroids, img_descriptors)\n",
    "    # add the computed BoW vector to the set of map representations\n",
    "    if bow_map_images is None:\n",
    "        bow_map_images = bow\n",
    "    else:\n",
    "        bow_map_images = np.vstack( (bow_map_images, bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the 1000 images in the map have been converted into their BoW representation. The data structure has size $NxK$, where $N=1000$ is the number of map images, while $K=32$ is the number of basic 'words' of the BoW representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow_map_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-score normalization\n",
    "In machine learning and data analysis problems, it is important to [normalize](https://en.wikipedia.org/wiki/Feature_scaling) (scale) the data, in order to avoid that features with larger range dominate the computation of distances (so skewing the computations). Here we use the Z-score normalization, meaning that we subtract the mean and divide by the variance (computed on the map images) of each feature (i.e. for each dimension of the BoW vectors). We use the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) library of Scikit.\n",
    "\n",
    "> __Note__: when computing the query BoW vectors, then, we need to also normalize them (using the mean and variance of the map vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "orig_bow_map_images = bow_map_images\n",
    "\n",
    "# Compute z-score statistics\n",
    "scaler = preprocessing.StandardScaler().fit(bow_map_images)\n",
    "# Normalize the vectors of the map collection (0 mean and 1 std)\n",
    "bow_map_images = scaler.transform(bow_map_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 02B: _Retrieve images_\n",
    "We are ready to retrieve images from the collection (i.e. map) images, given the BoW representation of a query image. In order to do that, compute the Euclidean distance between the BoW vector of the query image and the BoW vector of all the collection images and return those with the smaller distance. \n",
    "\n",
    "_Hint:_ after computing the distances between the query image BoW vector and all BoW vectors of the map image, you may want to sort them in ascending order; the Numpy function [argsort](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html) may turn out to be useful.\n",
    "\n",
    "> For this exercise you are asked to compute the Euclidean distance, but other distance metrics can also be used, e.g. Cosine distance, Minkowski Distance, Manhattan distance, etc. (this is left to you, if you want to explore them as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# receives as input the:\n",
    "#   - bag of words vectors of the map images\n",
    "#   - the bag of work vector of the query image\n",
    "def retrieve_images(map_bow_vectors, query_bow):\n",
    "    # n_map_bow_vectors = map_bow_vectors.shape[0]\n",
    "    # bow_distances = np.zeros(n_map_bow_vectors)\n",
    "    # most_similar = None  # use this to\n",
    "    \n",
    "    ## BEGIN ANSWER\n",
    "    idxs = np.argsort(np.linalg.norm(map_bow_vectors - query_bow, axis=1))\n",
    "    ## END ANSWER\n",
    "    \n",
    "    return idxs\n",
    "\n",
    "\n",
    "\n",
    "# Retrieve the most similar images to query image 221 (index 221-1=220)\n",
    "query_idx = 220\n",
    "img = plt.imread(\"data02/\" + q_imgs[query_idx])\n",
    "img = rgb2gray(img)\n",
    "# compute bag of words\n",
    "descriptor_extractor.detect_and_extract(img)  \n",
    "query_img_descriptors = descriptor_extractor.descriptors \n",
    "bow = bag_of_words(centroids, query_img_descriptors)\n",
    "\n",
    "# Normalize the query BoW vector using the mean and variance of the map (computed earlier and saved into the scaler object)\n",
    "bow = scaler.transform(bow.reshape(-1, 1).transpose())\n",
    "bow = bow.transpose().reshape(-1)\n",
    "\n",
    "# Retrieve the indices of the top-10 similar images from the map\n",
    "retrieved_images = retrieve_images(bow_map_images, bow)\n",
    "print('Indices of similar images retrieved: ', retrieved_images[:10])\n",
    "# Indices of the relevant map images for the query: we have the relevance judgements (Ground truth)\n",
    "relevant_images = np.where(sim[query_idx, :] == 1)[0]\n",
    "print('Indices of relevant images (given in the GT relevance judgements): ', relevant_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 02.C: _evaluation of performance_\n",
    "As now you are able to retrieve images from the map that are similar (according to the BoW framework) to query images, you should evaluate the performance.\n",
    "\n",
    "You can re-use the `precision_at_k()` function that you have developed for Assignment 01 (on text retrieval) and compute the precision@5 and precision@10 for the retrieval results of the previous exercise (`retrieved_images`).\n",
    "\n",
    "__Note__ performance might be very low. Do not worry about the results, this problem is very difficult and the implemented solution is a very simple one. Pay instead attention to the implementation and try to make insightful observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN ANSWER\n",
    "def precision_at_k(relevant, retrieved, k):\n",
    "    tp = np.sum(np.in1d(relevant, retrieved[:k]))\n",
    "    fp = len(relevant) - tp\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "prec5 = precision_at_k(relevant_images, retrieved_images, 5)\n",
    "prec10 = precision_at_k(relevant_images, retrieved_images, 10)\n",
    "## END ANSWER\n",
    "\n",
    "print('P@5: ', prec5)\n",
    "print('P@10: ', prec10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 02.D (Bonus): _overall performance evaluation_\n",
    "Extend the previous exercise by computing the Mean Average Precision (MAP) for the whole set of 500 query images.\n",
    "\n",
    "__Note:__ Implementation of this exercise is completely on your own, without a given code structure, but try to reuse as much as possible the code already available (or that you developed in Assignment 01).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN ANSWER\n",
    "## BEGIN ANSWER\n",
    "def average_precision(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    precisions = []\n",
    "    for k in range(len(retrieved)):\n",
    "        prec_at_k = precision_at_k(relevant, retrieved, k)\n",
    "        precisions.append(prec_at_k)\n",
    "    return sum(precisions)/len(precisions)\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    total = 0\n",
    "    count = len(all_retrieved)\n",
    "    for qid in range(len(all_retrieved)):\n",
    "        avg_precision = average_precision(all_relevant[qid], all_retrieved[qid])\n",
    "        total += avg_precision\n",
    "    # END ANSWER\n",
    "    return total / count\n",
    "\n",
    "all_relevant_images = []\n",
    "all_retrieved_images = []\n",
    "for query_idx in tqdm(range(len(q_imgs))):\n",
    "    img = plt.imread(\"data02/\" + q_imgs[query_idx])\n",
    "    img = rgb2gray(img)\n",
    "    # compute bag of words\n",
    "    descriptor_extractor.detect_and_extract(img)\n",
    "    query_img_descriptors = descriptor_extractor.descriptors\n",
    "    bow = bag_of_words(centroids, query_img_descriptors)\n",
    "\n",
    "    # Normalize the query BoW vector using the mean and variance of the map (computed earlier and saved into the scaler object)\n",
    "    bow = scaler.transform(bow.reshape(-1, 1).transpose())\n",
    "    bow = bow.transpose().reshape(-1)\n",
    "\n",
    "    # Retrieve the indices of the top-10 similar images from the map\n",
    "    retrieved_images = retrieve_images(bow_map_images, bow)\n",
    "#     print('Indices of similar images retrieved: ', retrieved_images[:10])\n",
    "    all_retrieved_images.append(retrieved_images)\n",
    "    # Indices of the relevant map images for the query: we have the relevance judgements (Ground truth)\n",
    "    relevant_images = np.where(sim[query_idx, :] == 1)[0]\n",
    "#     print('Indices of relevant images (given in the GT relevance judgements): ', relevant_images)\n",
    "    all_relevant_images.append(relevant_images)\n",
    "\n",
    "mapr = mean_average_precision(all_relevant_images, all_retrieved_images)\n",
    "# print(mapr)\n",
    "## END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(mapr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
