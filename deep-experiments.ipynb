{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import math\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "with h5py.File(\"data02/london_lite_gt.h5\",\"r\") as f:\n",
    "    fovs = f[\"fov\"][:]\n",
    "    sim = f[\"sim\"][:].astype(np.uint8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, json_path, transform=None, n=8):\n",
    "        self.transform=transform\n",
    "        self.n = n\n",
    "        self.root_dir = root_dir\n",
    "        with open(json_path,\"r\") as f:\n",
    "            m_idx = json.load(f)\n",
    "            self.m_imgs = np.array(m_idx[\"im_paths\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.m_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = plt.imread(os.path.join(self.root_dir, self.m_imgs[idx]))\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def find_global_features(loader, device, model):\n",
    "    global_features = None\n",
    "    for img_batch in tqdm(loader):\n",
    "        img_batch = img_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(img_batch)\n",
    "\n",
    "            # Pooling if model is densenet\n",
    "            if len(model) == 1:\n",
    "                output, _ = output.max(dim=2)  # Max pooling along the spatial dimensions (dim=2)\n",
    "                output, _ = output.max(dim=2)\n",
    "\n",
    "            if global_features is None:\n",
    "                global_features = output.cpu().numpy().squeeze()\n",
    "            else:\n",
    "                global_features = np.vstack((global_features, output.cpu().numpy().squeeze()))\n",
    "\n",
    "    return global_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def manhatten(matrix, vector):\n",
    "    return np.linalg.norm(matrix - vector, axis=1, ord=1)\n",
    "\n",
    "def infinity(matrix, vector):\n",
    "    return np.linalg.norm(matrix - vector, axis=1, ord=math.inf)\n",
    "\n",
    "def eucledian(matrix, vector):\n",
    "    return np.linalg.norm(matrix - vector, axis=1)\n",
    "\n",
    "def cosine(matrix, vector):\n",
    "    dists = np.zeros(len(vector))\n",
    "    for i in range(matrix.shape[0]):\n",
    "        dists[i] = np.dot(matrix[i], vector) / (np.linalg.norm(matrix[i]) * np.linalg.norm(vector))\n",
    "\n",
    "    return -dists"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def run_all_queries(device, model, distance_metric, bow_map_images, scaler):\n",
    "    q_database = CustomDataset(root_dir=\"data02\", json_path=\"data02/query/query_lite.json\", transform=transform)\n",
    "    all_relevant_images = []\n",
    "    all_retrieved_images = []\n",
    "    for query_idx in tqdm(range(len(q_database))):\n",
    "        img = q_database[query_idx]\n",
    "\n",
    "        # compute bag of words\n",
    "        with torch.no_grad():\n",
    "            img = img.to(device)\n",
    "            o = model(img[None, :])\n",
    "            # Pooling\n",
    "            if len(model) == 1:\n",
    "                o, _ = o.max(dim=2)  # Max pooling along the spatial dimensions (dim=2)\n",
    "                o, _ = o.max(dim=2)\n",
    "\n",
    "            repr = o.cpu().numpy().squeeze()\n",
    "\n",
    "        new_repr = scaler.transform(repr.reshape(-1, 1).transpose())\n",
    "        new_repr = new_repr.transpose().reshape(-1)\n",
    "\n",
    "        dists = distance_metric(bow_map_images, new_repr)\n",
    "        retrieved_images = np.argsort(np.array(dists))\n",
    "\n",
    "        all_retrieved_images.append(retrieved_images)\n",
    "        relevant_images = np.where(sim[query_idx, :] == 1)[0]\n",
    "        all_relevant_images.append(relevant_images)\n",
    "\n",
    "    return all_retrieved_images, all_relevant_images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def recall_at_k(relevant, retrieved, k):\n",
    "    return np.sum(np.in1d(relevant, retrieved[:k])) / len(relevant)\n",
    "\n",
    "def precision_at_k(relevant, retrieved, k):\n",
    "    tp = np.sum(np.in1d(relevant, retrieved[:k]))\n",
    "    return tp / k\n",
    "\n",
    "def average_precision(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    avg_prec = 0\n",
    "    for doc in relevant:\n",
    "        k = np.where(retrieved == doc)[0][0] + 1\n",
    "        avg_prec += precision_at_k(relevant, retrieved[:k], k)\n",
    "    return avg_prec/len(relevant)\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    total = 0\n",
    "    count = len(all_retrieved)\n",
    "\n",
    "    for qid in range(len(all_retrieved)):\n",
    "        avg_precision = average_precision(all_relevant[qid], all_retrieved[qid])\n",
    "        total += avg_precision\n",
    "    # END ANSWER\n",
    "    return total / count\n",
    "\n",
    "def average_recall_at_k(all_relevant, all_retrieved, k):\n",
    "    running_recall = 0\n",
    "    for relevant, retrieved in zip(all_relevant, all_retrieved):\n",
    "        r_k = recall_at_k(relevant, retrieved, k)\n",
    "        running_recall += r_k\n",
    "\n",
    "    return running_recall / len(all_relevant)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Loaded device: {device}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "densenet = models.densenet201(pretrained=True)\n",
    "densenet = torch.nn.Sequential(*list(densenet.children())[:-1])\n",
    "densenet = densenet.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "resnet = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))\n",
    "resnet = resnet.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6812b6aa22bf4bec83c5eacfe6c92e38"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kjwdamme/opt/anaconda3/envs/FIR/lib/python3.10/site-packages/torchvision/transforms/functional.py:152: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:212.)\n",
      "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((512, 512), antialias=False)\n",
    "])\n",
    "\n",
    "database = CustomDataset(root_dir=\"data02\", json_path=\"data02/database/database_lite.json\", transform=transform)\n",
    "loader = DataLoader(database, batch_size=64)\n",
    "\n",
    "for i, model in enumerate([densenet, resnet]):\n",
    "   for distance_metric in [manhatten, infinity, eucledian, cosine]:\n",
    "        global_features = find_global_features(loader, device, model)\n",
    "\n",
    "        # Compute z-score statistics\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        # Normalize the vectors of the map collection (0 mean and 1 std)\n",
    "        scaled_features = scaler.fit_transform(global_features)\n",
    "\n",
    "        all_retrieved_images, all_relevant_images = run_all_queries(device, model, distance_metric, global_features, scaler)\n",
    "\n",
    "        print(\"-------------\")\n",
    "        if i == 0:\n",
    "            print(\"Model: DenseNet202\")\n",
    "        else:\n",
    "            print(\"Model: ResNet101\")\n",
    "        print(f\"Distance metric: {distance_metric.__name__}\")\n",
    "        print(\"\")\n",
    "        mAP = mean_average_precision(all_relevant_images, all_retrieved_images)\n",
    "        print(f\"mAP: {mAP}\")\n",
    "        for k in [1, 5, 10]:\n",
    "            r_k = average_recall_at_k(all_relevant_images, all_retrieved_images, k)\n",
    "            print(f\"Recall@{k}: {r_k}\")\n",
    "        print(\"-------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
