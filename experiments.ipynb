{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SIFT Features, ORB Features, Deep\n",
    "# Cosine, euclidean, etc\n",
    "# BoW, VLAD\n",
    "# Differnt k (# of clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import SIFT, ORB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# map\n",
    "with open(\"data02/database/database_lite.json\", \"r\") as f:\n",
    "    m_idx = json.load(f)\n",
    "    m_imgs = np.array(m_idx[\"im_paths\"])\n",
    "    m_loc = np.array(m_idx[\"loc\"])\n",
    "\n",
    "# query\n",
    "with open(\"data02/query/query_lite.json\", \"r\") as f:\n",
    "    q_idx = json.load(f)\n",
    "    q_imgs = np.array(q_idx[\"im_paths\"])\n",
    "    q_loc = np.array(q_idx[\"loc\"])\n",
    "\n",
    "with h5py.File(\"data02/london_lite_gt.h5\", \"r\") as f:\n",
    "    fovs = f[\"fov\"][:]\n",
    "    sim = f[\"sim\"][:].astype(np.uint8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def extract_all_descriptors(extractor):\n",
    "    descriptors = None\n",
    "\n",
    "    for img_name in tqdm(m_imgs):\n",
    "        img = plt.imread(os.path.join('data02', img_name))\n",
    "        img = rgb2gray(img)\n",
    "\n",
    "        d = extractor(img)\n",
    "\n",
    "        if descriptors is None:\n",
    "            descriptors = d\n",
    "        else:\n",
    "            descriptors = np.vstack((descriptors, d))\n",
    "\n",
    "    return descriptors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def find_centroids(descriptors, k):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=5, verbose=0)\n",
    "    clusters = kmeans.fit(descriptors)\n",
    "    return clusters.cluster_centers_\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def manhatten(matrix, vector):\n",
    "    return np.linalg.norm(matrix - vector, axis=1, ord=1)\n",
    "\n",
    "def infinity(matrix, vector):\n",
    "    return np.linalg.norm(matrix - vector, axis=1, ord=math.inf)\n",
    "\n",
    "def eucledian(matrix, vector):\n",
    "    return np.linalg.norm(matrix - vector, axis=1)\n",
    "\n",
    "def cosine(matrix, vector):\n",
    "    dists = np.zeros(len(vector))\n",
    "    for i in range(matrix.shape[0]):\n",
    "        dists[i] = np.dot(matrix[i], vector) / (np.linalg.norm(matrix[i]) * np.linalg.norm(vector))\n",
    "\n",
    "    return -dists"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def extract_with_sift(image):\n",
    "    sift = SIFT()\n",
    "    sift.detect_and_extract(image)\n",
    "    return sift.descriptors\n",
    "\n",
    "def extract_with_orb(image):\n",
    "    orb = ORB()\n",
    "    orb.detect_and_extract(image)\n",
    "    return orb.descriptors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def bag_of_words(centroids, img_descriptors, distance_metric):\n",
    "    n_centroids = centroids.shape[0]  # number of centroids found with the KMeans clustering\n",
    "    n_descriptors = img_descriptors.shape[0]  # number of descriptors extracted from the image\n",
    "\n",
    "    bow_vector = np.zeros(n_centroids)\n",
    "\n",
    "    for i in range(n_descriptors):\n",
    "        dists = distance_metric(centroids, img_descriptors[i])\n",
    "        idx = np.argmin(dists)\n",
    "        bow_vector[idx] += 1\n",
    "    return bow_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def create_all_bow_vectors(extractor, centroids, distance_metric):\n",
    "    bow_map_images = None\n",
    "    for img_name in tqdm(m_imgs):\n",
    "        img = plt.imread(os.path.join('data02', img_name))\n",
    "        img = rgb2gray(img)\n",
    "\n",
    "        img_descriptors = extractor(img)  # descriptors (the feature vectors)\n",
    "\n",
    "        # compute BoW representation of the image (using the basic 'words', i.e. centroids, computed earlier)\n",
    "        bow = bag_of_words(centroids, img_descriptors, distance_metric)\n",
    "        # add the computed BoW vector to the set of map representations\n",
    "        if bow_map_images is None:\n",
    "            bow_map_images = bow\n",
    "        else:\n",
    "            bow_map_images = np.vstack( (bow_map_images, bow))\n",
    "\n",
    "    return bow_map_images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def recall_at_k(relevant, retrieved, k):\n",
    "    return np.sum(np.in1d(relevant, retrieved[:k])) / len(relevant)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def precision_at_k(relevant, retrieved, k):\n",
    "    tp = np.sum(np.in1d(relevant, retrieved[:k]))\n",
    "    return tp / k\n",
    "\n",
    "def average_precision(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    avg_prec = 0\n",
    "    for doc in relevant:\n",
    "        k = np.where(retrieved == doc)[0][0] + 1\n",
    "        avg_prec += precision_at_k(relevant, retrieved[:k], k)\n",
    "    return avg_prec/len(relevant)\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    total = 0\n",
    "    count = len(all_retrieved)\n",
    "\n",
    "    for qid in range(len(all_retrieved)):\n",
    "        avg_precision = average_precision(all_relevant[qid], all_retrieved[qid])\n",
    "        total += avg_precision\n",
    "    # END ANSWER\n",
    "    return total / count\n",
    "\n",
    "def average_recall_at_k(all_relevant, all_retrieved, k):\n",
    "    running_recall = 0\n",
    "    for relevant, retrieved in zip(all_relevant, all_retrieved):\n",
    "        r_k = recall_at_k(relevant, retrieved, k)\n",
    "        running_recall += r_k\n",
    "\n",
    "    return running_recall / len(all_relevant)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def run_all_queries(feature_extractor, centroids, distance_metric, scaler, bow_map_images):\n",
    "    all_relevant_images = []\n",
    "    all_retrieved_images = []\n",
    "    for query_idx in tqdm(range(len(q_imgs))):\n",
    "        img = plt.imread(os.path.join(\"data02\", q_imgs[query_idx]))\n",
    "        img = rgb2gray(img)\n",
    "        # compute bag of words\n",
    "        query_img_descriptors = feature_extractor(img)\n",
    "        bow = bag_of_words(centroids, query_img_descriptors, distance_metric)\n",
    "\n",
    "        bow = scaler.transform(bow.reshape(-1, 1).transpose())\n",
    "        bow = bow.transpose().reshape(-1)\n",
    "\n",
    "        # Retrieve the indices of the top-10 similar images from the map\n",
    "        # retrieved_images = np.argsort(np.linalg.norm(bow_map_images - bow, axis=1))\n",
    "        dists = distance_metric(bow_map_images, bow)\n",
    "\n",
    "        retrieved_images = np.argsort(np.array(dists))\n",
    "        # retrieved_images = np.argsort(eucledian(bow_map_images, bow))\n",
    "    #     print('Indices of similar images retrieved: ', retrieved_images[:10])\n",
    "        all_retrieved_images.append(retrieved_images)\n",
    "        # Indices of the relevant map images for the query: we have the relevance judgements (Ground truth)\n",
    "        relevant_images = np.where(sim[query_idx, :] == 1)[0]\n",
    "    #     print('Indices of relevant images (given in the GT relevance judgements): ', relevant_images)\n",
    "        all_relevant_images.append(relevant_images)\n",
    "\n",
    "    return all_retrieved_images, all_relevant_images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1152dc0f072a48908fae2b93875ebe90"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature_extractor in [extract_with_orb, extract_with_sift]:\n",
    "    descriptors = extract_all_descriptors(feature_extractor)\n",
    "    for n_centroids in [10, 30, 50]:\n",
    "        centroids = find_centroids(descriptors, n_centroids)\n",
    "\n",
    "        for distance_metric in [manhatten, infinity, cosine, eucledian]:\n",
    "            bow_map_images = create_all_bow_vectors(feature_extractor, centroids, distance_metric)\n",
    "\n",
    "            scaler = preprocessing.StandardScaler()\n",
    "            bow_map_images = scaler.fit_transform(bow_map_images)\n",
    "\n",
    "            all_retrieved_images, all_relevant_images = run_all_queries(feature_extractor, centroids, distance_metric, scaler, bow_map_images)\n",
    "\n",
    "            print(\"-------------\")\n",
    "            print(f\"Feature type: {feature_extractor.__name__}\")\n",
    "            print(f\"# of centroids: {n_centroids}\")\n",
    "            print(f\"Distance metric: {distance_metric.__name__}\")\n",
    "            print(\"\")\n",
    "            mAP = mean_average_precision(all_relevant_images, all_retrieved_images)\n",
    "            print(f\"mAP: {mAP}\")\n",
    "            for k in [1, 5, 10]:\n",
    "                r_k = average_recall_at_k(all_relevant_images, all_retrieved_images, k)\n",
    "                print(f\"Recall@{k}: {r_k}\")\n",
    "\n",
    "            print(\"-------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}