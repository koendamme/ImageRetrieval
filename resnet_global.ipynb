{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "with h5py.File(\"data02/london_lite_gt.h5\",\"r\") as f:\n",
    "    fovs = f[\"fov\"][:]\n",
    "    sim = f[\"sim\"][:].astype(np.uint8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, json_path, transform=None, n=8):\n",
    "        self.transform=transform\n",
    "        self.n = n\n",
    "        self.root_dir = root_dir\n",
    "        with open(json_path,\"r\") as f:\n",
    "            m_idx = json.load(f)\n",
    "            self.m_imgs = np.array(m_idx[\"im_paths\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.m_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = plt.imread(os.path.join(self.root_dir, self.m_imgs[idx]))\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Loaded device: {device}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kjwdamme/opt/anaconda3/envs/FIR/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/kjwdamme/opt/anaconda3/envs/FIR/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /Users/kjwdamme/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.densenet201(pretrained=True)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# o = model(torch.zeros((16, 3, 224, 224)))\n",
    "# # o = model(torch.zeros((64, 3, 512, 512)))\n",
    "# o.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "# model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "# model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "# model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "# model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# list(model.children())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((512, 512), antialias=False)\n",
    "    # transforms.Normalize(mean=0, std=1)\n",
    "])\n",
    "\n",
    "database = CustomDataset(root_dir=\"data02\", json_path=\"data02/database/database_lite.json\", transform=transform)\n",
    "loader = DataLoader(database, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finding global features\n",
    "global_features = None\n",
    "for img_batch in tqdm(loader):\n",
    "    img_batch = img_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img_batch)\n",
    "        # Pooling\n",
    "        output, _ = output.max(dim=2)  # Max pooling along the spatial dimensions (dim=2)\n",
    "        output, _ = output.max(dim=2)\n",
    "\n",
    "        if global_features is None:\n",
    "            global_features = output.cpu().numpy().squeeze()\n",
    "        else:\n",
    "            global_features = np.vstack((global_features, output.cpu().numpy().squeeze()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Compute z-score statistics\n",
    "scaler = preprocessing.StandardScaler().fit(global_features)\n",
    "# Normalize the vectors of the map collection (0 mean and 1 std)\n",
    "scaled_features = scaler.transform(global_features)\n",
    "\n",
    "np.mean(scaled_features), np.std(scaled_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cosine(map_bow_vectors, query_bow):\n",
    "    cosine_sim = np.zeros(map_bow_vectors.shape[0])\n",
    "\n",
    "    for i in range(map_bow_vectors.shape[0]):\n",
    "        cosine_sim[i] = np.dot(map_bow_vectors[i], query_bow) / (np.linalg.norm(map_bow_vectors[i]) * np.linalg.norm(query_bow))\n",
    "\n",
    "    return cosine_sim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def retrieve_images(map_bow_vectors, query_bow):\n",
    "    return np.argsort(np.linalg.norm(map_bow_vectors - query_bow, axis=1))\n",
    "    # return np.argsort(cosine(map_bow_vectors, query_bow))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## BEGIN ANSWER\n",
    "def precision_at_k(relevant, retrieved, k):\n",
    "    tp = np.sum(np.in1d(relevant, retrieved[:k]))\n",
    "    fp = len(relevant) - tp\n",
    "    return tp / k"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## BEGIN ANSWER\n",
    "## BEGIN ANSWER\n",
    "\n",
    "\n",
    "def average_precision(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    precisions = []\n",
    "    for k in range(1, len(retrieved)):\n",
    "        prec_at_k = precision_at_k(relevant, retrieved, k)\n",
    "        precisions.append(prec_at_k)\n",
    "    return sum(precisions)/len(precisions)\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    total = 0\n",
    "    count = len(all_retrieved)\n",
    "    for qid in range(len(all_retrieved)):\n",
    "        avg_precision = average_precision(all_relevant[qid], all_retrieved[qid])\n",
    "        total += avg_precision\n",
    "    # END ANSWER\n",
    "    return total / count\n",
    "\n",
    "q_database = CustomDataset(root_dir=\"data02\", json_path=\"data02/query/query_lite.json\", transform=transform)\n",
    "all_relevant_images = []\n",
    "all_retrieved_images = []\n",
    "for query_idx in tqdm(range(len(q_database))):\n",
    "    img = q_database[query_idx]\n",
    "\n",
    "    # compute bag of words\n",
    "    with torch.no_grad():\n",
    "        img = q_database[query_idx].to(device)\n",
    "        o = model(img[None, :])\n",
    "        # Pooling\n",
    "        o, _ = o.max(dim=2)  # Max pooling along the spatial dimensions (dim=2)\n",
    "        o, _ = o.max(dim=2)\n",
    "        print(o.shape)\n",
    "\n",
    "        repr = o.cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "    new_repr = scaler.transform(repr.reshape(-1, 1).transpose())\n",
    "    new_repr = new_repr.transpose().reshape(-1)\n",
    "\n",
    "    # Retrieve the indices of the top-10 similar images from the map\n",
    "    retrieved_images = retrieve_images(scaled_features, new_repr)\n",
    "    # retrieved_images = retrieve_images(global_features, repr)\n",
    "#     print('Indices of similar images retrieved: ', retrieved_images[:10])\n",
    "    all_retrieved_images.append(retrieved_images)\n",
    "    # Indices of the relevant map images for the query: we have the relevance judgements (Ground truth)\n",
    "    relevant_images = np.where(sim[query_idx, :] == 1)[0]\n",
    "#     print('Indices of relevant images (given in the GT relevance judgements): ', relevant_images)\n",
    "    all_relevant_images.append(relevant_images)\n",
    "\n",
    "mapr = mean_average_precision(all_relevant_images, all_retrieved_images)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mapr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
