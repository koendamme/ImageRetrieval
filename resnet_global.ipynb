{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 41,
=======
   "execution_count": 9,
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# with h5py.File(\"data02/london_lite_gt.h5\",\"r\") as f:\n",
    "#     fovs = f[\"fov\"][:]\n",
    "#     sim = f[\"sim\"][:].astype(np.uint8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
=======
   "execution_count": 10,
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "outputs": [],
   "source": [
    "with open('new_sim_10m.npy', 'rb') as f:\n",
    "    sim = np.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 44,
=======
   "execution_count": 11,
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, json_path, transform=None, n=8):\n",
    "        self.transform=transform\n",
    "        self.n = n\n",
    "        self.root_dir = root_dir\n",
    "        with open(json_path,\"r\") as f:\n",
    "            m_idx = json.load(f)\n",
    "            self.m_imgs = np.array(m_idx[\"im_paths\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.m_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = plt.imread(os.path.join(self.root_dir, self.m_imgs[idx]))\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 45,
=======
   "execution_count": 12,
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Loaded device: {device}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 46,
   "outputs": [],
=======
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kjwdamme/opt/anaconda3/envs/FIR/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/kjwdamme/opt/anaconda3/envs/FIR/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /Users/kjwdamme/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "source": [
    "# model = models.densenet201(pretrained=True)\n",
    "# model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "# model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 47,
=======
   "execution_count": null,
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "outputs": [],
   "source": [
    "# o = model(torch.zeros((16, 3, 224, 224)))\n",
    "# # o = model(torch.zeros((64, 3, 512, 512)))\n",
    "# o.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 47,
=======
   "execution_count": null,
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 48,
=======
   "execution_count": null,
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "outputs": [],
   "source": [
    "model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 49,
=======
   "execution_count": null,
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "outputs": [],
   "source": [
    "# model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "# model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "# model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 50,
=======
   "execution_count": null,
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "outputs": [],
   "source": [
    "# list(model.children())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 51,
=======
   "execution_count": null,
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((512, 512), antialias=False)\n",
    "    # transforms.Normalize(mean=0, std=1)\n",
    "])\n",
    "\n",
    "database = CustomDataset(root_dir=\"data02\", json_path=\"data02/database/database_lite.json\", transform=transform)\n",
    "loader = DataLoader(database, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "454c8c28305b4c22b9ce0d3f0929b410"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
=======
   "execution_count": null,
   "outputs": [],
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "source": [
    "# Finding global features\n",
    "global_features = None\n",
    "for img_batch in tqdm(loader):\n",
    "    img_batch = img_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img_batch)\n",
    "        # Pooling\n",
    "        output, _ = output.max(dim=2)  # Max pooling along the spatial dimensions (dim=2)\n",
    "        output, _ = output.max(dim=2)\n",
    "\n",
    "        if global_features is None:\n",
    "            global_features = output.cpu().numpy().squeeze()\n",
    "        else:\n",
    "            global_features = np.vstack((global_features, output.cpu().numpy().squeeze()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "(1.4901161e-09, 0.9999997)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "outputs": [],
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Compute z-score statistics\n",
    "scaler = preprocessing.StandardScaler().fit(global_features)\n",
    "# Normalize the vectors of the map collection (0 mean and 1 std)\n",
    "scaled_features = scaler.transform(global_features)\n",
    "\n",
    "np.mean(scaled_features), np.std(scaled_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 54,
=======
   "execution_count": null,
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "outputs": [],
   "source": [
    "def cosine(map_bow_vectors, query_bow):\n",
    "    cosine_sim = np.zeros(map_bow_vectors.shape[0])\n",
    "\n",
    "    for i in range(map_bow_vectors.shape[0]):\n",
    "        cosine_sim[i] = np.dot(map_bow_vectors[i], query_bow) / (np.linalg.norm(map_bow_vectors[i]) * np.linalg.norm(query_bow))\n",
    "\n",
    "    return cosine_sim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 55,
=======
   "execution_count": null,
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "outputs": [],
   "source": [
    "def retrieve_images(map_bow_vectors, query_bow):\n",
    "    return np.argsort(np.linalg.norm(map_bow_vectors - query_bow, axis=1))\n",
    "    # return np.argsort(cosine(map_bow_vectors, query_bow))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20: 0 / 1\n",
      "21: 0 / 2\n",
      "22: 0 / 2\n",
      "23: 1 / 2\n",
      "24: 1 / 2\n",
      "33: 0 / 3\n",
      "34: 0 / 3\n",
      "35: 0 / 4\n",
      "36: 0 / 4\n",
      "37: 0 / 2\n",
      "38: 0 / 2\n",
      "39: 0 / 1\n",
      "40: 0 / 1\n",
      "43: 0 / 1\n",
      "44: 0 / 1\n",
      "45: 0 / 1\n",
      "46: 0 / 1\n",
      "50: 0 / 1\n",
      "51: 0 / 1\n",
      "52: 0 / 4\n",
      "54: 0 / 5\n",
      "55: 0 / 7\n",
      "56: 0 / 8\n",
      "57: 0 / 7\n",
      "58: 0 / 7\n",
      "59: 0 / 7\n",
      "60: 0 / 13\n",
      "61: 0 / 6\n",
      "62: 0 / 2\n",
      "63: 1 / 1\n",
      "65: 1 / 1\n",
      "66: 0 / 1\n",
      "67: 1 / 2\n",
      "70: 1 / 2\n",
      "80: 0 / 1\n",
      "87: 0 / 1\n",
      "88: 0 / 2\n",
      "89: 0 / 2\n",
      "90: 0 / 2\n",
      "91: 0 / 2\n",
      "92: 0 / 3\n",
      "93: 0 / 3\n",
      "94: 0 / 3\n",
      "95: 1 / 3\n",
      "96: 0 / 3\n",
      "97: 0 / 3\n",
      "98: 0 / 3\n",
      "99: 0 / 2\n",
      "100: 0 / 3\n",
      "101: 0 / 2\n",
      "102: 0 / 2\n",
      "103: 0 / 2\n",
      "104: 0 / 2\n",
      "105: 0 / 2\n",
      "106: 0 / 2\n",
      "107: 0 / 2\n",
      "108: 0 / 3\n",
      "109: 0 / 3\n",
      "111: 0 / 2\n",
      "112: 0 / 2\n",
      "113: 0 / 2\n",
      "114: 0 / 2\n",
      "115: 0 / 2\n",
      "116: 0 / 2\n",
      "119: 0 / 1\n",
      "121: 1 / 3\n",
      "124: 2 / 3\n",
      "126: 0 / 1\n",
      "129: 1 / 1\n",
      "131: 3 / 4\n",
      "132: 2 / 2\n",
      "133: 4 / 4\n",
      "134: 1 / 3\n",
      "135: 1 / 2\n",
      "138: 0 / 3\n",
      "139: 0 / 2\n",
      "140: 0 / 1\n",
      "146: 0 / 2\n",
      "147: 0 / 2\n",
      "154: 0 / 1\n",
      "156: 0 / 1\n",
      "158: 0 / 1\n",
      "159: 0 / 2\n",
      "167: 0 / 2\n",
      "169: 0 / 1\n",
      "170: 0 / 1\n",
      "171: 0 / 2\n",
      "172: 1 / 3\n",
      "173: 1 / 1\n",
      "175: 1 / 2\n",
      "176: 1 / 1\n",
      "177: 1 / 2\n",
      "179: 1 / 2\n",
      "180: 0 / 2\n",
      "183: 0 / 1\n",
      "184: 0 / 1\n",
      "185: 1 / 2\n",
      "186: 1 / 1\n",
      "187: 2 / 2\n",
      "189: 1 / 1\n",
      "190: 1 / 1\n",
      "191: 0 / 1\n",
      "192: 1 / 1\n",
      "193: 0 / 1\n",
      "194: 0 / 1\n",
      "195: 0 / 2\n",
      "196: 0 / 1\n",
      "197: 0 / 2\n",
      "198: 0 / 3\n",
      "199: 0 / 1\n",
      "200: 0 / 1\n",
      "201: 0 / 2\n",
      "202: 0 / 2\n",
      "203: 1 / 2\n",
      "204: 0 / 1\n",
      "206: 0 / 2\n",
      "207: 0 / 3\n",
      "208: 0 / 3\n",
      "210: 0 / 1\n",
      "211: 0 / 1\n",
      "213: 0 / 1\n",
      "214: 0 / 2\n",
      "215: 0 / 1\n",
      "216: 0 / 3\n",
      "217: 0 / 3\n",
      "218: 0 / 3\n",
      "219: 0 / 3\n",
      "222: 0 / 3\n",
      "223: 0 / 3\n",
      "224: 0 / 3\n",
      "225: 0 / 3\n",
      "226: 0 / 1\n",
      "228: 0 / 2\n",
      "235: 0 / 1\n",
      "237: 0 / 2\n",
      "238: 0 / 2\n",
      "241: 0 / 1\n",
      "254: 0 / 1\n",
      "258: 0 / 1\n",
      "259: 0 / 1\n",
      "261: 0 / 2\n",
      "262: 0 / 1\n",
      "263: 0 / 1\n",
      "264: 0 / 1\n",
      "265: 0 / 1\n",
      "266: 0 / 2\n",
      "267: 0 / 2\n",
      "268: 0 / 2\n",
      "269: 1 / 1\n",
      "270: 1 / 1\n",
      "273: 0 / 1\n",
      "274: 1 / 3\n",
      "275: 1 / 2\n",
      "276: 0 / 2\n",
      "277: 0 / 3\n",
      "278: 0 / 1\n",
      "279: 1 / 1\n",
      "280: 0 / 1\n"
     ]
    }
   ],
   "source": [
    "q_database = CustomDataset(root_dir=\"data02\", json_path=\"data02/query/query_lite.json\", transform=transform)\n",
    "\n",
    "for query_idx in range(300):\n",
    "    img = q_database[query_idx]\n",
    "    with torch.no_grad():\n",
    "        img = q_database[query_idx].to(device)\n",
    "        o = model(img[None, :])\n",
    "        # Pooling\n",
    "        o, _ = o.max(dim=2)  # Max pooling along the spatial dimensions (dim=2)\n",
    "        o, _ = o.max(dim=2)\n",
    "\n",
    "        repr = o.cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "    new_repr = scaler.transform(repr.reshape(-1, 1).transpose())\n",
    "    new_repr = new_repr.transpose().reshape(-1)\n",
    "\n",
    "    # Retrieve the indices of the top-10 similar images from the map\n",
    "    retrieved_images = retrieve_images(scaled_features, new_repr)\n",
    "    # print('Indices of similar images retrieved: ', retrieved_images[:10])\n",
    "    # Indices of the relevant map images for the query: we have the relevance judgements (Ground truth)\n",
    "    relevant_images = np.where(sim[query_idx, :] == 1)[0]\n",
    "    # print('Indices of relevant images (given in the GT relevance judgements): ', relevant_images)\n",
    "    if len(relevant_images) > 0:\n",
    "        print(f\"{query_idx}: {len(set(retrieved_images[:100]).intersection(set(relevant_images)))} / {len(relevant_images)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
=======
   "execution_count": null,
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "outputs": [],
   "source": [
    "## BEGIN ANSWER\n",
    "def precision_at_k(relevant, retrieved, k):\n",
    "    tp = np.sum(np.in1d(relevant, retrieved[:k]))\n",
<<<<<<< HEAD
=======
    "    fp = len(relevant) - tp\n",
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
    "    return tp / k"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a1c1a58a4564b5eba2e9610f951862c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
=======
   "execution_count": null,
   "outputs": [],
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "source": [
    "## BEGIN ANSWER\n",
    "## BEGIN ANSWER\n",
    "\n",
    "\n",
    "def average_precision(relevant, retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    precisions = []\n",
    "    for k in range(1, len(retrieved)):\n",
    "        prec_at_k = precision_at_k(relevant, retrieved, k)\n",
    "        precisions.append(prec_at_k)\n",
    "    return sum(precisions)/len(precisions)\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):\n",
    "    # BEGIN ANSWER\n",
    "    total = 0\n",
    "    count = len(all_retrieved)\n",
    "    for qid in range(len(all_retrieved)):\n",
    "        avg_precision = average_precision(all_relevant[qid], all_retrieved[qid])\n",
    "        total += avg_precision\n",
    "    # END ANSWER\n",
    "    return total / count\n",
    "\n",
    "q_database = CustomDataset(root_dir=\"data02\", json_path=\"data02/query/query_lite.json\", transform=transform)\n",
    "all_relevant_images = []\n",
    "all_retrieved_images = []\n",
    "for query_idx in tqdm(range(len(q_database))):\n",
    "    img = q_database[query_idx]\n",
    "\n",
    "    # compute bag of words\n",
    "    with torch.no_grad():\n",
    "        img = q_database[query_idx].to(device)\n",
    "        o = model(img[None, :])\n",
    "        # Pooling\n",
    "        o, _ = o.max(dim=2)  # Max pooling along the spatial dimensions (dim=2)\n",
    "        o, _ = o.max(dim=2)\n",
    "        repr = o.cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "    new_repr = scaler.transform(repr.reshape(-1, 1).transpose())\n",
    "    new_repr = new_repr.transpose().reshape(-1)\n",
    "\n",
    "    relevant_images = np.where(sim[query_idx, :] == 1)[0]\n",
    "#     print('Indices of relevant images (given in the GT relevance judgements): ', relevant_images)\n",
    "\n",
    "    if len(relevant_images) > 0:\n",
    "        all_relevant_images.append(relevant_images)\n",
    "\n",
    "        # Retrieve the indices of the top-10 similar images from the map\n",
    "        retrieved_images = retrieve_images(scaled_features, new_repr)\n",
    "        # retrieved_images = retrieve_images(global_features, repr)\n",
    "    #     print('Indices of similar images retrieved: ', retrieved_images[:10])\n",
    "        all_retrieved_images.append(retrieved_images[:20])\n",
    "        # Indices of the relevant map images for the query: we have the relevance judgements (Ground truth)\n",
    "\n",
    "\n",
    "mapr = mean_average_precision(all_relevant_images, all_retrieved_images)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "0.00483819601696162"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "outputs": [],
>>>>>>> cd1dfa5f7481855067c056ded19dce911725513f
   "source": [
    "mapr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
